import React, { createContext, useContext, useEffect, useState } from 'react';
import { Audio } from 'expo-av';
import { Emotion } from '../types/emotion';
import Meyda from 'meyda';
import { createVAD } from '../utils/vad-webrtc';

interface AudioAnalysisContextType {
  startListening: () => Promise<void>;
  stopListening: () => Promise<void>;
  isListening: boolean;
  currentEmotion: Emotion | null;
}

const AudioAnalysisContext = createContext<AudioAnalysisContextType | undefined>(undefined);

// é¢„å®šä¹‰çš„çŒ«å’ªæƒ…æ„Ÿç‰¹å¾å‘é‡æ•°æ®åº“
const emotionFeatureDB = {
  comfortable: {
    features: [/* ... é¢„è®­ç»ƒçš„ç‰¹å¾å‘é‡ ... */],
    emotion: {
      id: 'comfortable',
      icon: 'ğŸ˜Œ',
      title: 'èˆ’é€‚',
      description: 'çŒ«å’ªæ„Ÿåˆ°èˆ’é€‚å’Œæ”¾æ¾ã€‚',
      categoryId: 'friendly',
    },
  },
  // ... å…¶ä»–æƒ…æ„Ÿç‰¹å¾
};

export function AudioAnalysisProvider({ children }: { children: React.ReactNode }) {
  const [isListening, setIsListening] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<Emotion | null>(null);
  const [vadModel, setVadModel] = useState<any>(null);
  const [recording, setRecording] = useState<Audio.Recording | null>(null);

  // VAD æ£€æµ‹åˆ°å£°éŸ³æ—¶çš„å¤„ç†
  const startRecording = async () => {
    if (recording) return;
    const newRecording = new Audio.Recording();
    try {
      await newRecording.prepareToRecordAsync(Audio.RecordingOptionsPresets.HIGH_QUALITY);
      await newRecording.startAsync();
      setRecording(newRecording);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  // VAD æ£€æµ‹åˆ°å£°éŸ³ç»“æŸæ—¶çš„å¤„ç†
  const stopRecording = async () => {
    if (!recording) return;
    try {
      await recording.stopAndUnloadAsync();
      const uri = recording.getURI();
      // è¿™é‡Œå¯ä»¥å¤„ç†å½•éŸ³æ–‡ä»¶ï¼Œä¾‹å¦‚è¿›è¡ŒéŸ³é¢‘åˆ†æ
      setRecording(null);
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  // åˆå§‹åŒ– VAD æ¨¡å‹
  useEffect(() => {
    const initVAD = async () => {
      const vad = await createVAD({
        onSpeechStart: () => {
          console.log('Cat sound detected!');
          startRecording();
        },
        onSpeechEnd: () => {
          console.log('Cat sound ended');
          stopRecording();
        },
        threshold: -40, // å¯ä»¥è°ƒæ•´è¿™ä¸ªå€¼æ¥æ”¹å˜çµæ•åº¦
        silenceTimeout: 500
      });
      
      setVadModel(vad);
    };

    initVAD();
    return () => {
      if (vadModel) {
        vadModel.destroy();
      }
    };
  }, []);

  // è®¡ç®—ç‰¹å¾å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
  const cosineSimilarity = (vec1: number[], vec2: number[]): number => {
    const dotProduct = vec1.reduce((acc, val, i) => acc + val * vec2[i], 0);
    const norm1 = Math.sqrt(vec1.reduce((acc, val) => acc + val * val, 0));
    const norm2 = Math.sqrt(vec2.reduce((acc, val) => acc + val * val, 0));
    return dotProduct / (norm1 * norm2);
  };

  // åˆ†æéŸ³é¢‘ç‰¹å¾
  const analyzeAudio = async (audioBuffer: AudioBuffer) => {
    // Convert AudioBuffer to Float32Array
    const audioData = audioBuffer.getChannelData(0);
    // Use Meyda to extract features
    const features = Meyda.extract(['mfcc', 'spectralCentroid'], audioData);
    
    // æ‰¾åˆ°æœ€åŒ¹é…çš„æƒ…æ„Ÿ
    let bestMatch = null;
    let highestSimilarity = -1;

    Object.entries(emotionFeatureDB).forEach(([_, data]) => {
      const similarity = cosineSimilarity(features as any, data.features);
      if (similarity > highestSimilarity) {
        highestSimilarity = similarity;
        bestMatch = data.emotion;
      }
    });

    if (bestMatch && highestSimilarity > 0.8) {
      setCurrentEmotion(bestMatch);
    }
  };

  const startListening = async () => {
    if (!vadModel) return;
    
    try {
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
      });
      
      await vadModel.start();
      setIsListening(true);
    } catch (error) {
      console.error('Failed to start listening:', error);
    }
  };

  const stopListening = async () => {
    if (!vadModel) return;
    
    try {
      await vadModel.stop();
      setIsListening(false);
    } catch (error) {
      console.error('Failed to stop listening:', error);
    }
  };

  return (
    <AudioAnalysisContext.Provider value={{
      startListening,
      stopListening,
      isListening,
      currentEmotion,
    }}>
      {children}
    </AudioAnalysisContext.Provider>
  );
}

export function useAudioAnalysis() {
  const context = useContext(AudioAnalysisContext);
  if (context === undefined) {
    throw new Error('useAudioAnalysis must be used within an AudioAnalysisProvider');
  }
  return context;
} 